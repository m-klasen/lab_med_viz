{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db57e0-abf3-47c6-83cf-603c084f05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spottunet.dataset.cc359 import *\n",
    "from spottunet.split import one2all\n",
    "from spottunet.torch.module.unet import UNet2D\n",
    "from spottunet.utils import sdice\n",
    "from dpipe.im.metrics import dice_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler \n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from monai import transforms as T\n",
    "from monai.transforms import Compose, apply_transform\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import json\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from dpipe.im.shape_ops import zoom\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "DEBUG=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11427e-96f7-417b-847d-26360ba56f54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Config & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145f862-280b-4ff1-b07a-cca51299f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from configs.config import CFG\n",
    "from utils import *\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "def class2str(f):\n",
    "    return [[name, getattr(f, name)] for name in dir(f) if not name.startswith('__')]\n",
    "\n",
    "def write_config(CFG):\n",
    "    with open(f\"{CFG.results_dir}/config.txt\", \"w\") as f:\n",
    "        for n,v in class2str(CFG):\n",
    "            f.write(f\"{n}={v} \\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90703324-98f7-48e1-a6d7-d73113efb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from dataset.dataloader import *\n",
    "from dataset.loader import PrefetchLoader, fast_collate\n",
    "from dataset.dataloader_utils import *\n",
    "\n",
    "from dataset.mixup import FastCollateMixup\n",
    "from dataset.augment import get_transforms, get_test_transforms\n",
    "\n",
    "from scheduler import LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf27fc-a709-45df-a5f9-8da595925215",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    from dataset.mixup import *\n",
    "\n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "    seed = 0xBadCafe\n",
    "    val_size = 4\n",
    "    n_experiments = len(cc359_df.fold.unique())\n",
    "    split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "    train_df = cc359_df.iloc[split[0][0]].reset_index()\n",
    "    valid_df = cc359_df.iloc[split[0][1]].reset_index()\n",
    "    \n",
    "\n",
    "    sa_x,sa_y = create_shared_arrays(CFG,train_df,root_dir=CFG.dataset_path)\n",
    "    valid_sa_x,valid_sa_y = create_3d_shared_arrays(CFG,valid_df,root_dir=CFG.dataset_path)\n",
    "    fcm_ma = create_shared_fcm_masks(CFG,train_df,root_dir=CFG.dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fcb0a5-3f52-4627-954d-02aac4e0929e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG.fcm_mask = \"gm\"\n",
    "if DEBUG:\n",
    "    train_dataset = CC359_Dataset(CFG,df=train_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=get_transforms(\"contrast_fcm\"),\n",
    "                                  mode=\"train\", cache=True, cached_x=sa_x, cached_y=sa_y, cached_fcm_mask=fcm_ma)\n",
    "    valid_dataset = CC359_Dataset(CFG,df=valid_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=get_test_transforms(),\n",
    "                                  mode=\"val\", cache=CFG.cache, cached_x=valid_sa_x,cached_y=valid_sa_y)\n",
    "\n",
    "    for x1,y in [train_dataset[1400]]:\n",
    "        \n",
    "        fig, (ax,bx) = plt.subplots(1,2, figsize=(20,10))\n",
    "        #out = transform(image=x.numpy().squeeze(),mask=y.numpy().squeeze())\n",
    "        #normed_img = out[\"image\"]\n",
    "        inp = ax.imshow(x1.numpy().squeeze())\n",
    "        plt.colorbar(inp, ax=ax)\n",
    "        \n",
    "\n",
    "        #tfms = T.MaskIntensity()\n",
    "        #fcm_masked_img = tfms(x.numpy().squeeze(),mask_data=y.numpy().squeeze())\n",
    "        #inv_masked_img = tfms(x.numpy().squeeze(),mask_data=~(y.numpy().astype(bool).squeeze()))\n",
    "        #Need to norm the non masked input too\n",
    "\n",
    "        bx.imshow(y.numpy().squeeze())\n",
    "        \n",
    "        plt.show()\n",
    "        #plt.hist(x.cpu().reshape(-1).numpy())\n",
    "        #plt.show()\n",
    "    for x4,y, id_ in [valid_dataset[1]]:\n",
    "        \n",
    "        print(x4.shape,x4.min(),x4.max(),y.max())\n",
    "        fig, (ax,bx) = plt.subplots(1,2, figsize=(20,10))\n",
    "        transform = get_transforms(\"default\")\n",
    "        #out = transform(image=x.numpy().squeeze(),mask=y.numpy().squeeze())\n",
    "        #normed_img = out[\"image\"]\n",
    "        inp = ax.imshow(x4[130].numpy().squeeze())\n",
    "        plt.colorbar(inp, ax=ax)\n",
    "        \n",
    "\n",
    "        tfms = T.MaskIntensity()\n",
    "        fcm_masked_img = tfms(x4[130].numpy().squeeze(),mask_data=y[130].numpy().squeeze())\n",
    "        inv_masked_img = tfms(x4[130].numpy().squeeze(),mask_data=~(y[130].numpy().astype(bool).squeeze()))\n",
    "        #Need to norm the non masked input too\n",
    "\n",
    "        bx.imshow(y[130].numpy().squeeze())\n",
    "        \n",
    "        plt.show()\n",
    "        #plt.hist(x.cpu().reshape(-1).numpy())\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607a14c-361e-4235-a0f8-21608f9da928",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.fcm_mask = None\n",
    "if DEBUG:\n",
    "    train_dataset = CC359_Dataset(CFG,df=train_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=get_transforms(\"default\"),\n",
    "                                  mode=\"train\", cache=True, cached_x=sa_x, cached_y=sa_y, cached_fcm_mask=fcm_ma)\n",
    "    valid_dataset = CC359_Dataset(CFG,df=valid_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=get_test_transforms(),\n",
    "                                  mode=\"val\", cache=CFG.cache, cached_x=valid_sa_x,cached_y=valid_sa_y)\n",
    "\n",
    "    for x2,y in [train_dataset[1400]]:\n",
    "        \n",
    "    \n",
    "        fig, (ax,bx) = plt.subplots(1,2, figsize=(20,10))\n",
    "        #out = transform(image=x.numpy().squeeze(),mask=y.numpy().squeeze())\n",
    "        #normed_img = out[\"image\"]\n",
    "        inp = ax.imshow(x2.numpy().squeeze())\n",
    "        plt.colorbar(inp, ax=ax)\n",
    "        \n",
    "\n",
    "        tfms = T.MaskIntensity()\n",
    "        fcm_masked_img = tfms(x.numpy().squeeze(),mask_data=y.numpy().squeeze())\n",
    "        inv_masked_img = tfms(x.numpy().squeeze(),mask_data=~(y.numpy().astype(bool).squeeze()))\n",
    "        #Need to norm the non masked input too\n",
    "\n",
    "        bx.imshow(y.numpy().squeeze())\n",
    "        \n",
    "        plt.show()\n",
    "        #plt.hist(x.cpu().reshape(-1).numpy())\n",
    "        #plt.show()\n",
    "    for x3,y, id_ in [valid_dataset[1]]:\n",
    "        \n",
    "        print(x3.shape,x3.min(),x3.max(),y.max())\n",
    "        fig, (ax,bx) = plt.subplots(1,2, figsize=(20,10))\n",
    "        transform = get_transforms(\"default\")\n",
    "        #out = transform(image=x.numpy().squeeze(),mask=y.numpy().squeeze())\n",
    "        #normed_img = out[\"image\"]\n",
    "        inp = ax.imshow(x3[130].numpy().squeeze())\n",
    "        plt.colorbar(inp, ax=ax)\n",
    "        \n",
    "\n",
    "        tfms = T.MaskIntensity()\n",
    "        fcm_masked_img = tfms(x3[130].numpy().squeeze(),mask_data=y[130].numpy().squeeze())\n",
    "        inv_masked_img = tfms(x3[130].numpy().squeeze(),mask_data=~(y[130].numpy().astype(bool).squeeze()))\n",
    "        #Need to norm the non masked input too\n",
    "\n",
    "        bx.imshow(y[130].numpy().squeeze())\n",
    "        \n",
    "        plt.show()\n",
    "        #plt.hist(x.cpu().reshape(-1).numpy())\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f4c87-3ed4-49c7-bb48-27324ccc198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax,bx) = plt.subplots(1,2, figsize=(20,10))\n",
    "#out = transform(image=x.numpy().squeeze(),mask=y.numpy().squeeze())\n",
    "#normed_img = out[\"image\"]\n",
    "tmp_diff = x1.numpy().squeeze() - x2.numpy().squeeze()\n",
    "tmp_diff2 = x3.numpy().squeeze() - x4.numpy().squeeze()\n",
    "print(tmp_diff.min(),tmp_diff.max())\n",
    "print(tmp_diff2.min(),tmp_diff2.max())\n",
    "inp = ax.imshow(tmp_diff)\n",
    "plt.colorbar(inp, ax=ax)\n",
    "inp = bx.imshow(tmp_diff2[130])\n",
    "plt.colorbar(inp, ax=bx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fdcc2-d2a0-4255-8698-652d6dc6ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    valid_df = cc359_df.iloc[split[0][1]].reset_index()\n",
    "\n",
    "    valid_sa_x,valid_sa_y = create_3d_shared_arrays(CFG,valid_df,root_dir=CFG.dataset_path)\n",
    "    valid_dataset = CC359_Dataset(CFG,df=valid_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=None, #get_test_transforms(),\n",
    "                                  mode=\"val\", cache=CFG.cache, cached_x=valid_sa_x,cached_y=valid_sa_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0a22a-4689-4e50-8613-3bbd319baa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    from dataset.rand_augment import *\n",
    "    aa_params = dict(\n",
    "                translate_const=int(256 * 0.20),\n",
    "                img_mean=tuple([0]),\n",
    "                #interpolation=str_to_pil_interp(interpolation)\n",
    "                )\n",
    "    for x,y,id_ in valid_dataset:\n",
    "\n",
    "        print(x.min(),x.max(),y.max())\n",
    "        fig, (ax,bx) = plt.subplots(1,2, figsize=(20,10))\n",
    "        inp = ax.imshow(x.cpu().squeeze().numpy()[130])\n",
    "        plt.colorbar(inp, ax=ax)\n",
    "        bx.imshow(y.cpu().squeeze().numpy()[130])\n",
    "        plt.show()\n",
    "        fig, (ax,bx) = plt.subplots(1,2, figsize=(20,10))\n",
    "        tfm_fct = solarize\n",
    "        tmp = Image.fromarray(x.cpu().squeeze().numpy()[130]*255).convert('L')\n",
    "        tmp2 = Image.fromarray(y.cpu().squeeze().numpy()[130]*255).convert('L')\n",
    "        tmp, tmp2 = tfm_fct(tmp,tmp2, _solarize_level_to_arg(9, aa_params))\n",
    "        inp = ax.imshow(np.array(tmp))\n",
    "        plt.colorbar(inp, ax=ax)\n",
    "        bx.imshow(y.cpu().squeeze().numpy()[130])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93800101-8c21-471e-a888-5e45cb8f0065",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d653cd4-6105-43af-9eb3-f66721d30c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from dataset.loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6276a6a-70c1-4f4d-80e2-3bffbb951a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_fold(fold):\n",
    "    result_dir = CFG.results_dir + \"/mode_\"+str(fold)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    #wandb.tensorboard.patch(root_logdir=result_dir+\"/logs\")\n",
    "    run = wandb.init(project=\"domain_shift\",\n",
    "                     group=CFG.model_name,\n",
    "                     name=f\"mode_{str(fold)}\",\n",
    "                     job_type=CFG.exp_name,\n",
    "                     config=class2dict(CFG),\n",
    "                     reinit=True,\n",
    "                     sync_tensorboard=True)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=result_dir+\"/logs\")\n",
    "    write_config(CFG)\n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "    \n",
    "    mixup_fn = None\n",
    "    if CFG.mixup:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=CFG.mixup, cutmix_alpha=CFG.cutmix, cutmix_minmax=None,\n",
    "            prob=CFG.mixup_prob, switch_prob=CFG.mixup_switch_prob, mode='batch',\n",
    "            label_smoothing=CFG.smoothing, num_classes=2)\n",
    "        collate_fn = FastCollateMixup(**mixup_args)\n",
    "    elif fast_collate:\n",
    "        collate_fn = fast_collate\n",
    "        \n",
    "    seed = 0xBadCafe\n",
    "    val_size = 4\n",
    "    n_experiments = len(cc359_df.fold.unique())\n",
    "    split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "\n",
    "\n",
    "    train_df = cc359_df.iloc[split[fold][0]].reset_index()\n",
    "    valid_df = cc359_df.iloc[split[fold][1]].reset_index()\n",
    "    test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "    sa_x = None; sa_y = None\n",
    "    valid_sa_x = None; valid_sa_y = None\n",
    "    if CFG.cache:\n",
    "        print(\"Caching Train Data ...\")\n",
    "        sa_x,sa_y = create_shared_arrays(CFG,train_df,root_dir=CFG.dataset_path)\n",
    "        valid_sa_x,valid_sa_y = create_3d_shared_arrays(CFG,valid_df,root_dir=CFG.dataset_path)\n",
    "    train_dataset = CC359_Dataset(CFG,df=train_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=get_transforms(CFG.tfms),\n",
    "                                  mode=\"train\", cache=CFG.cache, cached_x=sa_x, cached_y=sa_y)\n",
    "    \n",
    "    valid_dataset = CC359_Dataset(CFG,df=valid_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=get_test_transforms(),\n",
    "                                  mode=\"val\", cache=CFG.cache, cached_x=valid_sa_x,cached_y=valid_sa_y)\n",
    "    test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                                 voxel_spacing=CFG.voxel_spacing,\n",
    "                                  transforms=get_test_transforms(),mode=\"test\", cache=False)\n",
    "    \n",
    "    train_loader = PrefetchLoader(DataLoader(train_dataset,\n",
    "                                              batch_size=CFG.bs,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=CFG.num_workers,\n",
    "                                              sampler=None,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              pin_memory=False,\n",
    "                                              drop_last=True),\n",
    "                                  fp16=True)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=1,shuffle=False,\n",
    "                              num_workers=1,pin_memory=False)\n",
    "    test_dataloader = DataLoader(test_dataset, \n",
    "                                  batch_size=1,shuffle=False,\n",
    "                                  num_workers=1,pin_memory=False)\n",
    "\n",
    "    model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "    model.to(CFG.device)\n",
    "    \n",
    "    optim_dict = dict(optim=CFG.optim,lr=CFG.lr,weight_decay=CFG.wd)\n",
    "    optimizer = get_optimizer(model, **optim_dict)\n",
    "    \n",
    "    #scheduler = CFG.scheduler(optimizer, lr_lambda=lambda epoch: CFG.scheduler_multi_lr_fact )\n",
    "    if CFG.scheduler==torch.optim.lr_scheduler.OneCycleLR:\n",
    "        scheduler = CFG.scheduler(optimizer, max_lr=CFG.lr, steps_per_epoch=len(train_loader), epochs=CFG.epochs)\n",
    "    elif CFG.scheduler==LinearWarmupCosineAnnealingLR:\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(optimizer,\n",
    "                                                    warmup_epochs=CFG.warmup_epochs,\n",
    "                                                    max_epochs=CFG.epochs,\n",
    "                                                    warmup_start_lr=CFG.warmup_lr)\n",
    "    else:\n",
    "        print(\"no scheduler selected\")\n",
    "    criterion = CFG.crit    \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"from torch_lr_finder import LRFinder\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=100, num_iter=100)\n",
    "    lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "    lr_finder.reset()\"\"\"\n",
    "\n",
    "    \n",
    "    trainer = Trainer(CFG,\n",
    "                      model=model, \n",
    "                      device=CFG.device, \n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      criterion=criterion,\n",
    "                      writer=writer,\n",
    "                      fold=fold,\n",
    "                      max_norm=CFG.max_norm,\n",
    "                      mixup_fn=mixup_fn)\n",
    "    \n",
    "    history = trainer.fit(\n",
    "            CFG.epochs, \n",
    "            train_loader, \n",
    "            valid_loader, \n",
    "            f\"{result_dir}/\", \n",
    "            CFG.epochs,\n",
    "        )\n",
    "    trainer.test(test_dataloader,result_dir)\n",
    "    td_sdice = get_target_domain_metrics(CFG.dataset_path,Path(CFG.results_dir),fold)\n",
    "    #writer.add_hparams(class2dict(CFG),td_sdice)\n",
    "    wandb.log(td_sdice)\n",
    "    writer.close()\n",
    "    run.finish()\n",
    "\n",
    "    del trainer\n",
    "    del train_loader\n",
    "    del valid_loader\n",
    "    del train_dataset\n",
    "    del valid_dataset\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc1c9e-939c-40d2-93e0-4e5d36197e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fold in CFG.fold:\n",
    "    run_fold(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c5e41-036b-4f0a-a5ca-f99e01e17206",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cef89-6bad-4c60-9566-7ab41f71a910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hausdorff import hausdorff_distance\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def test_run(fold):\n",
    "    result_dir = CFG.results_dir + \"/mode_\"+str(fold)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "\n",
    "    model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "    model.load_state_dict(torch.load(f\"{result_dir}/e_39.pth\")[\"model_state_dict\"])\n",
    "    #model.load_state_dict(torch.load(f\"baseline_results/baseline_focal_lovasz_adam_default/mode_{fold}/e_39.pth\")[\"model_state_dict\"])\n",
    "    \n",
    "    model.to(CFG.device)\n",
    "\n",
    "\n",
    "    seed = 0xBadCafe\n",
    "    val_size = 4\n",
    "    n_experiments = len(cc359_df.fold.unique())\n",
    "    split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "\n",
    "    test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "    test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=get_test_transforms(),\n",
    "                                  mode=\"test\", cache=False)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, \n",
    "                                  batch_size=1,shuffle=False,\n",
    "                                  num_workers=0,pin_memory = False)\n",
    "\n",
    "    bs=256\n",
    "    model.eval()\n",
    "    metrics = {'sdice_score': sdice, 'dice_score': dice_score}\n",
    "\n",
    "    results = defaultdict(dict)\n",
    "    for step,(x,y,_id) in progress_bar(enumerate(test_dataloader, 1), len(test_dataloader)):\n",
    "        y = y.squeeze(0)#.to(torch.int64)\n",
    "        with torch.no_grad():\n",
    "            x = x[0].to(CFG.device); _id=_id[0]\n",
    "            c,h,w = x.shape\n",
    "            outputs = []\n",
    "            for idx in range(0,c,bs):\n",
    "                out = model(x[idx:min(idx+bs,c)].unsqueeze(1))\n",
    "                out = out.squeeze().sigmoid().cpu().detach().numpy()\n",
    "                if len(out.shape)==2: out = out[None,:,:]\n",
    "                outputs.append(out)\n",
    "            outputs = np.concatenate(outputs)#.transpose(1,0,2,3)\n",
    "            outputs = (outputs > .5).squeeze()\n",
    "            y = (y > .5).numpy().squeeze()\n",
    "\n",
    "        results['sdice_score'][_id] = sdice(y, outputs, CFG.voxel_spacing,CFG.sdice_tolerance)\n",
    "        results['dice_score'][_id]  = dice_score(y, outputs)\n",
    "        results['hausdorff'][_id]  = np.mean([hausdorff_distance(y_,o_) for o_,y_ in zip(np.uint8(outputs), np.uint8(y))])\n",
    "        #def mp_hausdorf(i):\n",
    "        #    return hausdorff_distance(np.uint8(outputs[i,...]),np.uint8(y[i,...]))\n",
    "\n",
    "        #with Pool(5) as p:\n",
    "        #    print(p.map(mp_hausdorf, np.arange(outputs.shape[0])))\n",
    "        #break\n",
    "    #with open(os.path.join(result_dir, 'sdice_score' + '.json'), 'w') as f:\n",
    "    #    json.dump(results['sdice_score'], f, indent=0)\n",
    "    #with open(os.path.join(result_dir, 'dice_score'+ '.json'), 'w') as f:\n",
    "    #    json.dump(results['dice_score'], f, indent=0)\n",
    "    with open(os.path.join(result_dir, 'hausdorff'+ '.json'), 'w') as f:\n",
    "        json.dump(results['hausdorff'], f, indent=0)\n",
    "    td_sdice = get_target_domain_metrics(CFG.dataset_path,Path(CFG.results_dir),fold)\n",
    "    return td_sdice   \n",
    "\n",
    "#for fold in CFG.fold:\n",
    "CFG.results_dir = \"baseline_results/baseline_focal_lovasz_adam_default/\"\n",
    "for fold in [0,2,3,4,5]:\n",
    "    test_run(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381229c5-8fa5-450c-9a5f-49935ee9eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 1\n",
    "result_dir = CFG.results_dir + \"/mode_\"+str(fold)\n",
    "td_sdice = get_target_domain_metrics(CFG.dataset_path,Path(CFG.results_dir),fold)\n",
    "td_sdice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2353f-81e2-4cbf-9887-dfb5f151986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_target_domain_metrics(CFG.dataset_path,Path(CFG.results_dir),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b418a98-f6c6-4311-8ead-3f2c34e59096",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_target_domain_metrics(CFG.dataset_path,Path(\"baseline_results/baseline_lovasz_default\"),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16affb16-c1f5-458b-881b-e257d5fee160",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616e245-936e-4b0c-bf9f-87126ef0134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai import transforms as T\n",
    "\n",
    "ttas = [None,\n",
    "        #T.ShiftIntensityd(keys=(\"image\"), offset=-0.1),\n",
    "        #T.ShiftIntensityd(keys=(\"image\"), offset=0.1), \n",
    "        #T.Flipd(keys=(\"image\",\"seg\"), spatial_axis=0, allow_missing_keys=False)\n",
    "       ]\n",
    "\n",
    "norm = T.NormalizeIntensityd(keys=(\"image\"), subtrahend=(0.122), divisor=(0.224))\n",
    "\n",
    "def apply_tta(img, segm, tta):\n",
    "    if tta!=None:\n",
    "        tfms = T.Compose([tta,\n",
    "                          #norm\n",
    "                         ])\n",
    "    else:\n",
    "        tfms = T.Compose([\n",
    "                          #norm\n",
    "        ])\n",
    "        \n",
    "    img = img[None,:,:,:]\n",
    "    segm = segm[None,:,:,:]\n",
    "    tfmed = tfms({'image':img, 'seg':segm})       \n",
    "    img = tfmed['image'].squeeze()\n",
    "    segm = tfmed['seg'].squeeze()\n",
    "        \n",
    "\n",
    "    \n",
    "    return img, segm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e8c2d-72f0-41ef-8aa3-877e6f33d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=3\n",
    "slice_index = 130\n",
    "CFG.transpose = (2,0,1)\n",
    "CFG.fcm_mask = None #\"csf\"\n",
    "result_dir = \"baseline_results/baseline_focal_lovasz_adam_default\" + f\"/mode_{fold}/\"\n",
    "output_dir =  \"predictions/baseline_focal_lovasz_adam_default/\" + f\"/mode_{fold}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "\n",
    "model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "model.load_state_dict(torch.load(f\"{result_dir}/e_39.pth\")[\"model_state_dict\"])\n",
    "model.to(CFG.device)\n",
    "\n",
    "\n",
    "seed = 0xBadCafe\n",
    "val_size = 4\n",
    "n_experiments = len(cc359_df.fold.unique())\n",
    "split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                             voxel_spacing=CFG.voxel_spacing,transforms=get_transforms(\"default\"),\n",
    "                             mode=\"test\", cache=False)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                              batch_size=1,shuffle=False,\n",
    "                              num_workers=0,pin_memory = False)\n",
    "\n",
    "def id_to_scanner(id):\n",
    "    df = test_df[test_df['id']==id]\n",
    "    return df['tomograph_model'].values[0] + str(df['tesla_value'].values[0])\n",
    "\n",
    "bs=16    \n",
    "model.eval()\n",
    "import copy\n",
    "for step,(x,y,_id) in progress_bar(enumerate(test_dataloader, 1), len(test_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        _id=_id[0]\n",
    "        if _id==\"CC0002\":\n",
    "            for tta in ttas:\n",
    "                x_c, y_c = copy.deepcopy(x), copy.deepcopy(y)\n",
    "                x_c, y_c = apply_tta(x_c,y_c,tta)\n",
    "                x_c = x_c.to(CFG.device); \n",
    "                c,h,w = x_c.shape\n",
    "                outputs = []\n",
    "                for idx in range(0,c,bs):\n",
    "                    out = model(x_c[idx:min(idx+bs,c)].unsqueeze(1))\n",
    "                    out = out.squeeze().sigmoid().cpu().detach().numpy()\n",
    "                    if len(out.shape)==2: out = out[None,:,:]\n",
    "                    outputs.append(out)\n",
    "                logits = np.concatenate(outputs)\n",
    "                outputs = (logits > .5)\n",
    "                surface_dice = sdice(y_c.squeeze().numpy().astype(bool), outputs, CFG.voxel_spacing,CFG.sdice_tolerance)\n",
    "                fig, (ax, bx, cx, dx) = plt.subplots(1,4, figsize=(20,5))\n",
    "                inp = ax.imshow(x_c[slice_index].cpu().numpy())\n",
    "                ax.set_title(str(_id)+\" \"+id_to_scanner(_id))\n",
    "                fig.colorbar(inp, ax=ax)\n",
    "                bx.imshow(outputs[slice_index])\n",
    "                bx.set_title(str(round(surface_dice, 4)))\n",
    "                cx.imshow(y_c[slice_index].squeeze())\n",
    "                dx.imshow(y_c[slice_index].squeeze()-outputs[slice_index])\n",
    "                plt.show()\n",
    "            np.save(output_dir+f\"/{_id}.npy\",outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41d784-8f3a-45e9-85ff-f7c722f9a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=3\n",
    "result_dir = \"baseline_results/baseline_lovasz_sideways_default\" + f\"/mode_{fold}/\"\n",
    "output_dir =  \"predictions/baseline_lovasz_sideways_default/\" + f\"/mode_{fold}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "\n",
    "model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "model.load_state_dict(torch.load(f\"{result_dir}/e_39.pth\")[\"model_state_dict\"])\n",
    "model.to(CFG.device)\n",
    "\n",
    "\n",
    "seed = 0xBadCafe\n",
    "val_size = 4\n",
    "n_experiments = len(cc359_df.fold.unique())\n",
    "split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "\n",
    "test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                             voxel_spacing=CFG.voxel_spacing,\n",
    "                              transforms=None,mode=\"test\", cache=False)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                              batch_size=1,shuffle=False,\n",
    "                              num_workers=0,pin_memory = False)\n",
    "\n",
    "bs=16    \n",
    "model.eval()\n",
    "import copy\n",
    "for step,(x,y,_id) in progress_bar(enumerate(test_dataloader, 1), len(test_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for tta in ttas:\n",
    "            x_c, y_c = copy.deepcopy(x), copy.deepcopy(y)\n",
    "            x_c, y_c = apply_tta(x_c,y_c,tta)\n",
    "            x_c = x_c.to(CFG.device); _id=_id[0]\n",
    "            c,h,w = x_c.shape\n",
    "            outputs = []\n",
    "            for idx in range(0,c,bs):\n",
    "                out = model(x_c[idx:min(idx+bs,c)].unsqueeze(1))\n",
    "                out = out.squeeze().cpu().detach().numpy()\n",
    "                if len(out.shape)==2: out = out[None,:,:]\n",
    "                outputs.append(out)\n",
    "            logits = np.concatenate(outputs)\n",
    "            outputs = (logits > .5)\n",
    "            surface_dice = sdice(y_c.squeeze().numpy().astype(bool), outputs, CFG.voxel_spacing,CFG.sdice_tolerance)\n",
    "            fig, (ax, bx, cx, dx) = plt.subplots(1,4, figsize=(20,5))\n",
    "            inp = ax.imshow(x_c[130].cpu().numpy())\n",
    "            #ax.set_title(str(tta))\n",
    "            fig.colorbar(inp, ax=ax)\n",
    "            lgts = bx.imshow(logits[130])\n",
    "            fig.colorbar(lgts, ax=bx)\n",
    "            cx.imshow(outputs[130])\n",
    "            dx.imshow(y_c[130].squeeze())\n",
    "            plt.title(str(round(surface_dice, 4)))\n",
    "            plt.show()\n",
    "    #np.save(output_dir+f\"/{_id}.npy\",outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb895075-34e1-48a3-9c42-54a7886b57f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0172f0-a005-45a7-a69f-3b795fee1288",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dice Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a969cf-5d3a-4727-9ce9-c5a0d51a8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(f\"meta.csv\",delimiter=\",\", index_col='id')\n",
    "meta.head()\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    \"\"\"Load the contents of a json file.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def get_sdices(path_base, path_oracle=None):\n",
    "    records = []\n",
    "    for s in sorted(meta['fold'].unique()):\n",
    "        res_row = {}\n",
    "\n",
    "        # one2all results:\n",
    "        try:\n",
    "            sdices = load_json(path_base / f'mode_{s}/sdice_score.json')\n",
    "        except:\n",
    "            sdices = dict()\n",
    "        for t in sorted(set(meta['fold'].unique()) - {s}):\n",
    "            df_row = meta[meta['fold'] == t].iloc[0]\n",
    "            target_name = df_row['tomograph_model'] + str(df_row['tesla_value'])\n",
    "\n",
    "            ids_t = meta[meta['fold'] == t].index\n",
    "            res_row[target_name] = np.mean([sdsc for _id, sdsc in sdices.items() if _id in ids_t])\n",
    "        df_row = meta[meta['fold'] == s].iloc[0]\n",
    "        source_name = df_row['tomograph_model'] + str(df_row['tesla_value'])\n",
    "        sdices = {}\n",
    "        if path_oracle:\n",
    "            for n_val in range(3):\n",
    "                try:\n",
    "                    sdices = {**sdices,\n",
    "                              **load_json(path_oracle / f'mode_{s * 3 + n_val}/sdice_score.json')}\n",
    "                except:\n",
    "                    None\n",
    "            res_row[source_name] = np.mean(list(sdices.values()))\n",
    "\n",
    "        res_row[' '] = source_name\n",
    "        records.append(res_row)\n",
    "    return records\n",
    "\n",
    "def get_dice(path_base, path_oracle=None):\n",
    "    records = []\n",
    "    for s in sorted(meta['fold'].unique()):\n",
    "        res_row = {}\n",
    "\n",
    "        # one2all results:\n",
    "        try:\n",
    "            sdices = load_json(path_base / f'mode_{s}/dice_score.json')\n",
    "        except:\n",
    "            sdices = dict()\n",
    "        #sdices = dict(sorted(sdices.items()))\n",
    "        for t in sorted(set(meta['fold'].unique()) - {s}):\n",
    "            df_row = meta[meta['fold'] == t].iloc[0]\n",
    "            target_name = df_row['tomograph_model'] + str(df_row['tesla_value'])\n",
    "\n",
    "            ids_t = meta[meta['fold'] == t].index\n",
    "            res_row[target_name] = np.mean([sdsc for _id, sdsc in sdices.items() if _id in ids_t])\n",
    "        df_row = meta[meta['fold'] == s].iloc[0]\n",
    "        source_name = df_row['tomograph_model'] + str(df_row['tesla_value'])\n",
    "        sdices = {}\n",
    "        if path_oracle:\n",
    "            for n_val in range(3):\n",
    "                try:\n",
    "                    sdices = {**sdices,\n",
    "                              **load_json(path_oracle / f'mode_{s * 3 + n_val}/dice_score.json')}\n",
    "                except:\n",
    "                    None\n",
    "            res_row[source_name] = np.mean(list(sdices.values()))\n",
    "\n",
    "        res_row[' '] = source_name\n",
    "        records.append(res_row)\n",
    "    return records\n",
    "\n",
    "def get_hausdorff(path_base, path_oracle=None):\n",
    "    records = []\n",
    "    for s in sorted(meta['fold'].unique()):\n",
    "        res_row = {}\n",
    "\n",
    "        # one2all results:\n",
    "        try:\n",
    "            sdices = load_json(path_base / f'mode_{s}/hausdorff.json')\n",
    "        except:\n",
    "            sdices = dict()\n",
    "        #sdices = dict(sorted(sdices.items()))\n",
    "        for t in sorted(set(meta['fold'].unique()) - {s}):\n",
    "            df_row = meta[meta['fold'] == t].iloc[0]\n",
    "            target_name = df_row['tomograph_model'] + str(df_row['tesla_value'])\n",
    "\n",
    "            ids_t = meta[meta['fold'] == t].index\n",
    "            res_row[target_name] = np.mean([sdsc for _id, sdsc in sdices.items() if _id in ids_t])\n",
    "        df_row = meta[meta['fold'] == s].iloc[0]\n",
    "        source_name = df_row['tomograph_model'] + str(df_row['tesla_value'])\n",
    "        sdices = {}\n",
    "        if path_oracle:\n",
    "            for n_val in range(3):\n",
    "                try:\n",
    "                    sdices = {**sdices,\n",
    "                              **load_json(path_oracle / f'mode_{s * 3 + n_val}/sdice_score.json')}\n",
    "                except:\n",
    "                    None\n",
    "            res_row[source_name] = np.mean(list(sdices.values()))\n",
    "\n",
    "        res_row[' '] = source_name\n",
    "        records.append(res_row)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3aee5f-7253-47ec-8b54-690fee267fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dpipe.io import load\n",
    "\n",
    "path_base = Path('baseline_results/baseline_focal_lovasz_adam_default')\n",
    "oracle_path = Path('oracle_results/focal_lovasz_adam_default_None')\n",
    "\n",
    "records = get_sdices(path_base, oracle_path)\n",
    "df = pd.DataFrame.from_records(records, index=' ')\n",
    "df[df.index]\n",
    "print(df.mean().mean())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df[df.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"Surface Dice Score\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()\n",
    "\n",
    "\n",
    "records = get_dice(path_base,oracle_path)\n",
    "df = pd.DataFrame.from_records(records, index=' ')\n",
    "df[df.index]\n",
    "print(df.mean().mean())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df[df.index], annot=True, vmin=0.8,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"Dice Score\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()\n",
    "\n",
    "records = get_sdices(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice TD mean: \",tmp.mean().mean())\n",
    "records = get_sdices(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice Oracle mean: \",tmp.mean().mean())\n",
    "\n",
    "records = get_dice(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice TD mean: \",tmp.mean().mean())\n",
    "records = get_dice(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice Oracle mean: \",tmp.mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd09ee-b461-4010-a43f-24e17d314f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('baseline_results/baseline_focal_lovasz_adam_default')\n",
    "oracle_path = Path('oracle_results/focal_lovasz_adam_default_None')\n",
    "\n",
    "records = get_hausdorff(path_base)\n",
    "df_hd = pd.DataFrame.from_records(records, index=' ')\n",
    "df_hd[df_hd.index]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df_hd[df_hd.index], annot=True, vmin=1,vmax=5.0, annot_kws={'size': 15}, cmap=\"Blues\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccda2c1-03ba-4edf-a054-ee6ee034b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('baseline_results/baseline_lovasz_nostopping_default')\n",
    "records = get_sdices(path_base)\n",
    "df3 = pd.DataFrame.from_records(records, index=' ')\n",
    "df3[df3.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e72691-cd37-40f3-af8b-bc59a7fbeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('baseline_results/baseline_focal_lovasz_adam_rand_aug_default_v1')\n",
    "oracle_path = Path('oracle_results/focal_lovasz_adam_rand_aug_default')\n",
    "records = get_dice(path_base, oracle_path)\n",
    "df2 = pd.DataFrame.from_records(records, index=' ')\n",
    "df2[df2.index]\n",
    "print(df2.mean().mean())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "bx = sns.heatmap(df2[df2.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"Dice Score\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "records = get_sdices(path_base, oracle_path)\n",
    "df2 = pd.DataFrame.from_records(records, index=' ')\n",
    "df2[df2.index]\n",
    "print(df2.mean().mean())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "bx = sns.heatmap(df2[df2.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"Surface Dice Score\")\n",
    "plt.show()\n",
    "\n",
    "records = get_sdices(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice TD mean: \",tmp.mean().mean())\n",
    "records = get_sdices(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice Oracle mean: \",tmp.mean().mean())\n",
    "\n",
    "records = get_dice(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice TD mean: \",tmp.mean().mean())\n",
    "records = get_dice(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice Oracle mean: \",tmp.mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63e3c5-bf53-4036-90c4-477760125d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('predictions/baseline_focal_lovasz_multiview_ensemble')\n",
    "oracle_path = Path('oracle_results/baseline_focal_lovasz_multiview_ensemble')\n",
    "\n",
    "\n",
    "records = get_dice(path_base, oracle_path)\n",
    "df2 = pd.DataFrame.from_records(records, index=' ')\n",
    "df2[df2.index]\n",
    "print(df2.mean().mean())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "bx = sns.heatmap(df2[df2.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"Dice Scores\")\n",
    "plt.show()\n",
    "\n",
    "records = get_sdices(path_base, oracle_path)\n",
    "df_hd = pd.DataFrame.from_records(records, index=' ')\n",
    "df_hd[df_hd.index]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df_hd[df_hd.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"Surface Dice Scores\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()\n",
    "\n",
    "records = get_sdices(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice TD mean: \",tmp.mean().mean())\n",
    "records = get_sdices(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice Oracle mean: \",tmp.mean().mean())\n",
    "\n",
    "records = get_dice(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice TD mean: \",tmp.mean().mean())\n",
    "records = get_dice(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice Oracle mean: \",tmp.mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55993a8d-66e5-4d0d-ac61-c8fbb7d2ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('predictions/baseline_focal_lovasz_multiview_ensemble')\n",
    "oracle_path = Path('oracle_results/baseline_focal_lovasz_multiview_ensemble')\n",
    "\n",
    "records = get_sdices(path_base, oracle_path)\n",
    "df_hd = pd.DataFrame.from_records(records, index=' ')\n",
    "df_hd[df_hd.index]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df_hd[df_hd.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()\n",
    "\n",
    "records = get_sdices(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice TD mean: \",tmp.mean().mean())\n",
    "records = get_sdices(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice Oracle mean: \",tmp.mean().mean())\n",
    "\n",
    "records = get_dice(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice TD mean: \",tmp.mean().mean())\n",
    "records = get_dice(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice Oracle mean: \",tmp.mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06d69c-816a-4e22-a0f4-d550befcb3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('baseline_results/baseline_focal_lovasz_SGD_None_None')\n",
    "records = get_dice(path_base)\n",
    "df4 = pd.DataFrame.from_records(records, index=' ')\n",
    "df4[df4.index]\n",
    "print(df4.mean().mean())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df4[df4.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"SGD Baseline\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()\n",
    "\n",
    "records = get_sdices(path_base)\n",
    "df4 = pd.DataFrame.from_records(records, index=' ')\n",
    "df4[df4.index]\n",
    "print(df4.mean().mean())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df4[df4.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"SGD Baseline\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()\n",
    "\n",
    "records = get_sdices(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice TD mean: \",tmp.mean().mean())\n",
    "records = get_sdices(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Sdice Oracle mean: \",tmp.mean().mean())\n",
    "\n",
    "records = get_dice(path_base)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice TD mean: \",tmp.mean().mean())\n",
    "records = get_dice(None,oracle_path)\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"dice Oracle mean: \",tmp.mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b320e31-0433-4cbb-8dc1-0e9efa85e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('baseline_results/baseline_focal_lovasz_SGD_default_None')\n",
    "records = get_sdices(path_base)\n",
    "df5 = pd.DataFrame.from_records(records, index=' ')\n",
    "df5[df5.index]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df5[df5.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"SGD Baseline\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d46f74-e365-4d7e-ae9b-f32eb1bfa2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('baseline_results/baseline_focal_lovasz_SGD_rand_aug_default')\n",
    "records = get_dice(path_base)\n",
    "df5 = pd.DataFrame.from_records(records, index=' ')\n",
    "df5[df5.index]\n",
    "print(df5.mean().mean())\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df5[df5.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"SGD Rand Aug.\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()\n",
    "\n",
    "records = get_sdices(path_base)\n",
    "df5 = pd.DataFrame.from_records(records, index=' ')\n",
    "df5[df5.index]\n",
    "print(df5.mean().mean())\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df5[df5.index], annot=True, vmin=0.5,vmax=1.0, annot_kws={'size': 15})\n",
    "plt.title(\"SGD Rand Aug.\")\n",
    "plt.tick_params(axis = 'x', labelsize = 12) # x font label size\n",
    "plt.tick_params(axis = 'y', labelsize = 12) # y font label size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f5f17-774b-457d-a601-888e70aa92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = get_sdices(Path('baseline_results/baseline_focal_lovasz_SGD_None_None'))\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"SGD TD mean: \",tmp.mean().mean())\n",
    "records = get_sdices(Path('baseline_results/baseline_focal_lovasz_SGD_rand_aug_default'))\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"SGD TD mean: \",tmp.mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d7189-a37c-4521-9c9e-1ab37b6a728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_path = Path('oracle_results/focal_lovasz_adam_default_None')\n",
    "\n",
    "\n",
    "records = get_sdices(Path('baseline_results/baseline_focal_lovasz_adam_default'))\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Adam TD mean: \",tmp.mean().mean())\n",
    "\n",
    "records = get_sdices(Path('baseline_results/baseline_focal_lovasz_adam_rand_aug_default_v1'))\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Adam AUG TD mean: \",tmp.mean().mean())#\n",
    "\n",
    "records = get_sdices(Path('predictions/baseline_focal_lovasz_multiview_ensemble'))\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Adam AUG MVE TD mean: \",tmp.mean().mean())#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f7536-d1c8-40fe-900d-04d0ae612c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = get_sdices(None,Path('oracle_results/focal_lovasz_adam_default_None'))\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Adam TD mean: \",tmp.mean().mean())\n",
    "\n",
    "records = get_sdices(None,Path('oracle_results/focal_lovasz_adam_rand_aug_default'))\n",
    "tmp = pd.DataFrame.from_records(records, index=' ')\n",
    "print(\"Adam AUG TD mean: \",tmp.mean().mean())#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99877f59-be3c-4d5a-8daf-9ef515558d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.heatmap(df3[df3.index], annot=True, vmin=0.5,vmax=1.0)\n",
    "plt.title(\"Old Baseline NoAug\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8389d54-b5b3-4d8d-9620-51eacc97ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df2-df)[df2.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbffdb4-f9d5-43f6-88b3-abc8d4e9af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940c09b-1289-43bd-bc38-cb839ee3d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8055838119624629\n",
    "0.7694199297374601"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch1.9] *",
   "language": "python",
   "name": "conda-env-torch1.9-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
