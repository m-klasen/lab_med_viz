{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2eda7f7-d32b-4573-90a0-5ca29fcc0738",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TODOs\n",
    "\n",
    "- [x] Inference Loop\n",
    "- [ ] Oracle?\n",
    "- [ ] Finetune\n",
    "- [ ] Spottune\n",
    "- [x] Voxelization NaN to num\n",
    "- [x] Precompute 3rd order sample voxel spacing upfront\n",
    "\n",
    "\n",
    "#### Augs?\n",
    "- [ ] LAMB\n",
    "- [ ] Label Smoothing\n",
    "- [ ] Stoch. depth\n",
    "- [ ] CutMix / MixUp\n",
    "- [ ] Hflip? (prob not)\n",
    "- [ ] RandomResizedCrop\n",
    "- [ ] Rand Augment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47db57e0-abf3-47c6-83cf-603c084f05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spottunet.dataset.cc359 import *\n",
    "from spottunet.split import one2all\n",
    "from spottunet.torch.module.unet import UNet2D\n",
    "from spottunet.utils import sdice\n",
    "from dpipe.im.metrics import dice_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler \n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from monai import transforms as T\n",
    "from monai.transforms import Compose, apply_transform\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import json\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from dpipe.im.shape_ops import zoom\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11427e-96f7-417b-847d-26360ba56f54",
   "metadata": {},
   "source": [
    "### Config & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5145f862-280b-4ff1-b07a-cca51299f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from configs.config import CFG\n",
    "from utils import *\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90703324-98f7-48e1-a6d7-d73113efb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataloader import *\n",
    "from dataset.loader import *\n",
    "from dataset.dataloader_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40bf27fc-a709-45df-a5f9-8da595925215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\\nseed = 0xBadCafe\\nval_size = 4\\nn_experiments = len(cc359_df.fold.unique())\\nsplit = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\\ntrain_df = cc359_df.iloc[split[0][0]].reset_index()\\n\\ntrain_dataset = CC359_Dataset(df=train_df,root_dir=CFG.dataset_path,\\n                              voxel_spacing=CFG.voxel_spacing,transforms=tfms,\\n                              mode=\"train\", cache=True)\\n\\nvalid_df = cc359_df.iloc[split[0][1]].reset_index()\\n\\nvalid = CC359_Dataset(df=valid_df,root_dir=CFG.dataset_path,\\n                              voxel_spacing=CFG.voxel_spacing,transforms=None,\\n                              mode=\"val\", cache=False)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "seed = 0xBadCafe\n",
    "val_size = 4\n",
    "n_experiments = len(cc359_df.fold.unique())\n",
    "split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "train_df = cc359_df.iloc[split[0][0]].reset_index()\n",
    "\n",
    "train_dataset = CC359_Dataset(df=train_df,root_dir=CFG.dataset_path,\n",
    "                              voxel_spacing=CFG.voxel_spacing,transforms=tfms,\n",
    "                              mode=\"train\", cache=True)\n",
    "\n",
    "valid_df = cc359_df.iloc[split[0][1]].reset_index()\n",
    "\n",
    "valid = CC359_Dataset(df=valid_df,root_dir=CFG.dataset_path,\n",
    "                              voxel_spacing=CFG.voxel_spacing,transforms=None,\n",
    "                              mode=\"val\", cache=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3301dd-9b1b-4433-8aba-80adddfc31d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x,y,_id = next(iter(valid))\\nplt.imshow(x[180].squeeze(), \"gray\")\\nplt.show()\\nplt.imshow(y[180])\\nplt.show()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"x,y,_id = next(iter(valid))\n",
    "plt.imshow(x[180].squeeze(), \"gray\")\n",
    "plt.show()\n",
    "plt.imshow(y[180])\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fcb0a5-3f52-4627-954d-02aac4e0929e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x,y = next(iter(train_dataset))\\nplt.imshow(train_dataset.shared_array[520], \"gray\")\\nplt.show()\\nplt.imshow(train_dataset.shared_array2[520])\\nplt.show()'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"x,y = next(iter(train_dataset))\n",
    "plt.imshow(train_dataset.shared_array[520], \"gray\")\n",
    "plt.show()\n",
    "plt.imshow(train_dataset.shared_array2[520])\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93800101-8c21-471e-a888-5e45cb8f0065",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d653cd4-6105-43af-9eb3-f66721d30c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from dataset.loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6276a6a-70c1-4f4d-80e2-3bffbb951a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_fold(fold):\n",
    "    result_dir = CFG.results_dir + \"/mode_\"+str(fold)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    #wandb.tensorboard.patch(root_logdir=result_dir+\"/logs\")\n",
    "    run = wandb.init(project=\"domain_shift\",\n",
    "                     group=CFG.model_name,\n",
    "                     name=f\"mode_{str(fold)}\",\n",
    "                     job_type=\"rand_slice3\",\n",
    "                     config=class2dict(CFG),\n",
    "                     reinit=True,\n",
    "                     sync_tensorboard=True)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=result_dir+\"/logs\")\n",
    "    \n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "    \n",
    "\n",
    "    #model = smp.Unet(encoder_name=\"resnet50\", encoder_weights=\"imagenet\", in_channels=CFG.n_chans_in,classes=CFG.n_chans_out)\n",
    "\n",
    "    model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "    model.to(CFG.device)\n",
    "    \n",
    "    #optimizer = CFG.optim(model.parameters(),lr=CFG.lr,weight_decay=CFG.wd)\n",
    "    optimizer = CFG.optim(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd, betas=(.9, .999), adam=False)\n",
    "    #scheduler = CFG.scheduler(optimizer, lr_lambda=lambda epoch: CFG.scheduler_multi_lr_fact )\n",
    "    criterion = CFG.crit\n",
    "\n",
    "    seed = 0xBadCafe\n",
    "    val_size = 4\n",
    "    n_experiments = len(cc359_df.fold.unique())\n",
    "    split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "\n",
    "\n",
    "    train_df = cc359_df.iloc[split[fold][0]].reset_index()\n",
    "    valid_df = cc359_df.iloc[split[fold][1]].reset_index()\n",
    "    test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "    print(\"Caching Train Data ...\")\n",
    "    \n",
    "    sa_x,sa_y = create_shared_arrays(CFG,train_df,root_dir=CFG.dataset_path)\n",
    "    train_dataset = CC359_Dataset(CFG,df=train_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=tfms,\n",
    "                                  mode=\"train\", cache=True, cached_x=sa_x, cached_y=sa_y)\n",
    "    \n",
    "    valid_dataset = CC359_Dataset(CFG,df=valid_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,\n",
    "                                  transforms=None,mode=\"val\", cache=False)\n",
    "    test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                                 voxel_spacing=CFG.voxel_spacing,\n",
    "                                  transforms=None,mode=\"test\", cache=False)\n",
    "    \n",
    "    train_loader = PrefetchLoader(DataLoader(train_dataset,\n",
    "                                              batch_size=CFG.bs,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=CFG.num_workers,\n",
    "                                              sampler=None,\n",
    "                                              collate_fn=fast_collate,\n",
    "                                              pin_memory=False,\n",
    "                                              drop_last=True),\n",
    "                                  fp16=CFG.fp16)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=1,shuffle=False,\n",
    "                              num_workers=1,pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, \n",
    "                                  batch_size=1,shuffle=False,\n",
    "                                  num_workers=1,pin_memory=False)\n",
    "\n",
    "    from torch_lr_finder import LRFinder\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=100, num_iter=100)\n",
    "    lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "    lr_finder.reset()\n",
    "    \n",
    "    \"\"\"trainer = Trainer(CFG,\n",
    "                      model=model, \n",
    "                      device=CFG.device, \n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      criterion=criterion,\n",
    "                      writer=writer,\n",
    "                      fold=fold,\n",
    "                      max_norm=CFG.max_norm)\n",
    "    \n",
    "    history = trainer.fit(\n",
    "            CFG.epochs, \n",
    "            train_loader, \n",
    "            valid_loader, \n",
    "            f\"{result_dir}/\", \n",
    "            CFG.epochs,\n",
    "        )\n",
    "    trainer.test(test_dataloader,result_dir)\n",
    "    td_sdice = get_target_domain_metrics(CFG.dataset_path,Path(CFG.results_dir),fold)\n",
    "    #writer.add_hparams(class2dict(CFG),td_sdice)\n",
    "    wandb.log(td_sdice)\n",
    "    writer.close()\n",
    "    run.finish()\n",
    "\n",
    "    del trainer\n",
    "    del train_loader\n",
    "    del valid_loader\n",
    "    del train_dataset\n",
    "    del valid_dataset\n",
    "    gc.collect()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc1c9e-939c-40d2-93e0-4e5d36197e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fold in CFG.fold:\n",
    "    run_fold(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09a469-0824-4c12-9e20-c7e85cb44ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedeb3c-1438-4901-af0e-24975bed71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "model.to(CFG.device)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"baseline6/mode_0/e1-loss3.043.pth\")[\"model_state_dict\"])\n",
    "\n",
    "cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "seed = 0xBadCafe\n",
    "val_size = 4\n",
    "n_experiments = len(cc359_df.fold.unique())\n",
    "split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734f50e-7a62-424a-9e7f-7e168c6e85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cc359_df.iloc[split[fold][0]].reset_index()\n",
    "\n",
    "print(\"Caching Train Data ...\")\n",
    "\n",
    "sa_x,sa_y = create_shared_arrays(train_df,root_dir=CFG.dataset_path)\n",
    "train_dataset = CC359_Dataset(df=train_df,root_dir=CFG.dataset_path,\n",
    "                              voxel_spacing=CFG.voxel_spacing,transforms=tfms,\n",
    "                              mode=\"train\", cache=True, cached_x=sa_x, cached_y=sa_y)\n",
    "\n",
    "\n",
    "train_loader = PrefetchLoader(DataLoader(train_dataset,\n",
    "                                              batch_size=CFG.bs,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=CFG.num_workers,\n",
    "                                              sampler=None,\n",
    "                                              collate_fn=fast_collate,\n",
    "                                              pin_memory=False,\n",
    "                                              drop_last=True),\n",
    "                                  fp16=CFG.fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cef89-6bad-4c60-9566-7ab41f71a910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_run(fold):\n",
    "    result_dir = CFG.results_dir + \"/mode_\"+str(fold)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "\n",
    "    model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "    #model = smp.Unet(encoder_name=\"resnet50\", encoder_weights=\"swsl\", in_channels=CFG.n_chans_in,classes=CFG.n_chans_out)\n",
    "    model.load_state_dict(torch.load(f\"{result_dir}/mode_{fold}_best_epoch_model.pth\")[\"model_state_dict\"])\n",
    "    \n",
    "    model.to(CFG.device)\n",
    "\n",
    "\n",
    "    seed = 0xBadCafe\n",
    "    val_size = 4\n",
    "    n_experiments = len(cc359_df.fold.unique())\n",
    "    split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "\n",
    "    test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "    test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=None,\n",
    "                                  mode=\"test\", cache=False)\n",
    "    trainer = Trainer(CFG,model,CFG.device, None,None,None,None,fold,CFG.max_norm)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, \n",
    "                                  batch_size=1,shuffle=False,\n",
    "                                  num_workers=0,pin_memory = False)\n",
    "\n",
    "    trainer.test(test_dataloader, result_dir)\n",
    "    \n",
    "#for fold in CFG.fold:\n",
    "test_run(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a3aee5f-7253-47ec-8b54-690fee267fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'siemens3': 0.8862069239613072, 'ge15': 0.7441230970897291, 'ge3': 0.8912394862975026, 'philips15': 0.8970851070655189, 'philips3': 0.8286721599408774}\n",
      "{'siemens15': 0.793814857791286, 'ge15': 0.5653082755803079, 'ge3': 0.8225196980954298, 'philips15': 0.8261902887021051, 'philips3': 0.7110882322227293}\n",
      "{'siemens15': 0.8891796363359588, 'siemens3': 0.8272426106209404, 'ge3': 0.8278275451776236, 'philips15': 0.910006792778337, 'philips3': 0.8410205023556002}\n",
      "{'siemens15': 0.7821464758318191, 'siemens3': 0.7399309079510205, 'ge15': 0.5524823019288387, 'philips15': 0.8087147676361184, 'philips3': 0.4670436739161707}\n",
      "{'siemens15': 0.8873857290765282, 'siemens3': 0.8701553290248403, 'ge15': 0.734153952054563, 'ge3': 0.8284189264340683, 'philips3': 0.7408041083601975}\n",
      "{'siemens15': 0.8689110423814996, 'siemens3': 0.846955769289682, 'ge15': 0.863288075647754, 'ge3': 0.8199882577969172, 'philips15': 0.8481256074157847}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siemens15</th>\n",
       "      <th>siemens3</th>\n",
       "      <th>ge15</th>\n",
       "      <th>ge3</th>\n",
       "      <th>philips15</th>\n",
       "      <th>philips3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>siemens15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886207</td>\n",
       "      <td>0.744123</td>\n",
       "      <td>0.891239</td>\n",
       "      <td>0.897085</td>\n",
       "      <td>0.828672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>siemens3</th>\n",
       "      <td>0.793815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.565308</td>\n",
       "      <td>0.822520</td>\n",
       "      <td>0.826190</td>\n",
       "      <td>0.711088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ge15</th>\n",
       "      <td>0.889180</td>\n",
       "      <td>0.827243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.827828</td>\n",
       "      <td>0.910007</td>\n",
       "      <td>0.841021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ge3</th>\n",
       "      <td>0.782146</td>\n",
       "      <td>0.739931</td>\n",
       "      <td>0.552482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.808715</td>\n",
       "      <td>0.467044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>philips15</th>\n",
       "      <td>0.887386</td>\n",
       "      <td>0.870155</td>\n",
       "      <td>0.734154</td>\n",
       "      <td>0.828419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.740804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>philips3</th>\n",
       "      <td>0.868911</td>\n",
       "      <td>0.846956</td>\n",
       "      <td>0.863288</td>\n",
       "      <td>0.819988</td>\n",
       "      <td>0.848126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           siemens15  siemens3      ge15       ge3  philips15  philips3\n",
       "                                                                       \n",
       "siemens15        NaN  0.886207  0.744123  0.891239   0.897085  0.828672\n",
       "siemens3    0.793815       NaN  0.565308  0.822520   0.826190  0.711088\n",
       "ge15        0.889180  0.827243       NaN  0.827828   0.910007  0.841021\n",
       "ge3         0.782146  0.739931  0.552482       NaN   0.808715  0.467044\n",
       "philips15   0.887386  0.870155  0.734154  0.828419        NaN  0.740804\n",
       "philips3    0.868911  0.846956  0.863288  0.819988   0.848126       NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dpipe.io import load\n",
    "\n",
    "path_base = Path('baseline_results/baseline16')\n",
    "\n",
    "meta = pd.read_csv(f\"meta.csv\",delimiter=\",\", index_col='id')\n",
    "meta.head()\n",
    "\n",
    "records = []\n",
    "for s in sorted(meta['fold'].unique()):\n",
    "    res_row = {}\n",
    "    \n",
    "    # one2all results:\n",
    "    sdices = load(path_base / f'mode_{s}/sdice_score.json')\n",
    "    #sdices = dict(sorted(sdices.items()))\n",
    "    for t in sorted(set(meta['fold'].unique()) - {s}):\n",
    "        df_row = meta[meta['fold'] == t].iloc[0]\n",
    "        target_name = df_row['tomograph_model'] + str(df_row['tesla_value'])\n",
    "        \n",
    "        ids_t = meta[meta['fold'] == t].index\n",
    "        res_row[target_name] = np.mean([sdsc for _id, sdsc in sdices.items() if _id in ids_t])\n",
    "    print(res_row)\n",
    "    df_row = meta[meta['fold'] == s].iloc[0]\n",
    "    source_name = df_row['tomograph_model'] + str(df_row['tesla_value'])\n",
    "    sdices = {}\n",
    "    #for n_val in range(3):\n",
    "    #    sdices = {**sdices,\n",
    "    #              **load(path_base / f'mode_{s}/sdice_score.json')}\n",
    "    #res_row[source_name] = np.mean(list(sdices.values()))\n",
    "\n",
    "    res_row[' '] = source_name\n",
    "    records.append(res_row)\n",
    "df = pd.DataFrame.from_records(records, index=' ')\n",
    "df[df.index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-torch1.9]",
   "language": "python",
   "name": "conda-env-anaconda3-torch1.9-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
