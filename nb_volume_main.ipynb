{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902d3b08-64ba-4e9c-b56e-42acb4849801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spottunet.dataset.cc359 import *\n",
    "from spottunet.split import one2all\n",
    "from spottunet.utils import sdice\n",
    "from dpipe.im.metrics import dice_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler \n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from monai import transforms as T\n",
    "from monai.transforms import Compose, apply_transform\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import json\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from dpipe.im.shape_ops import zoom\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from monai.networks.nets import UNETR, UNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch.cuda.set_device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b02e001-1d9e-4305-80fa-292e52bb1023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e0810f-ead2-4617-86aa-686630e35566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from configs.volume_config import CFG\n",
    "from utils import *\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "def class2str(f):\n",
    "    return [[name, getattr(f, name)] for name in dir(f) if not name.startswith('__')]\n",
    "\n",
    "def write_config(CFG):\n",
    "    with open(f\"{CFG.results_dir}/config.txt\", \"w\") as f:\n",
    "        for n,v in class2str(CFG):\n",
    "            f.write(f\"{n}={v} \\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18abd32a-18e3-4077-b055-510d39de7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataset.dataloader_3d import *\n",
    "from dataset.loader import PrefetchLoader, fast_collate\n",
    "from dataset.dataloader_utils import *\n",
    "\n",
    "from dataset.mixup import FastCollateMixup\n",
    "from dataset.augment import get_transforms, get_test_transforms\n",
    "\n",
    "from volume_trainer import Trainer\n",
    "from dataset.loader import *\n",
    "\n",
    "from scheduler import LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026bd22b-a0da-4a29-9984-8c82cf45b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(fold):\n",
    "    result_dir = CFG.results_dir + \"/mode_\"+str(fold)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    #wandb.tensorboard.patch(root_logdir=result_dir+\"/logs\")\n",
    "    run = wandb.init(project=\"domain_shift\",\n",
    "                     group=CFG.model_name,\n",
    "                     name=f\"mode_{str(fold)}\",\n",
    "                     job_type=f\"baseline_{CFG.tfms}\",\n",
    "                     config=class2dict(CFG),\n",
    "                     reinit=True,\n",
    "                     sync_tensorboard=True)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=result_dir+\"/logs\")\n",
    "    write_config(CFG)\n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "    \n",
    "    mixup_fn = None\n",
    "    if CFG.mixup:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=CFG.mixup, cutmix_alpha=CFG.cutmix, cutmix_minmax=None,\n",
    "            prob=CFG.mixup_prob, switch_prob=CFG.mixup_switch_prob, mode='batch',\n",
    "            label_smoothing=CFG.smoothing, num_classes=2)\n",
    "        collate_fn = FastCollateMixup(**mixup_args)\n",
    "    elif fast_collate:\n",
    "        collate_fn = fast_collate\n",
    "        \n",
    "    seed = 0xBadCafe\n",
    "    val_size = 4\n",
    "    n_experiments = len(cc359_df.fold.unique())\n",
    "    split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "\n",
    "\n",
    "    train_df = cc359_df.iloc[split[fold][0]].reset_index()\n",
    "    valid_df = cc359_df.iloc[split[fold][1]].reset_index()\n",
    "    test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "    \n",
    "    train_sa_x, train_sa_y = None, None\n",
    "    valid_sa_x, valid_sa_y = None, None\n",
    "    if CFG.cache:\n",
    "        print(\"Caching Train Data ...\")\n",
    "        train_sa_x,train_sa_y = create_3d_shared_arrays(CFG,train_df,root_dir=CFG.dataset_path)\n",
    "        valid_sa_x,valid_sa_y = create_3d_shared_arrays(CFG,valid_df,root_dir=CFG.dataset_path)\n",
    "    train_dataset = CC359_Dataset(CFG,df=train_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=None,\n",
    "                                  mode=\"train\", cache=CFG.cache, cached_x=train_sa_x, cached_y=train_sa_y)\n",
    "    \n",
    "    valid_dataset = CC359_Dataset(CFG,df=valid_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing, transforms=None,\n",
    "                                  mode=\"val\", cache=False)\n",
    "    test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                                 voxel_spacing=CFG.voxel_spacing,\n",
    "                                  transforms=None,mode=\"test\", cache=False)\n",
    "    \n",
    "    train_loader = PrefetchLoader(DataLoader(train_dataset,\n",
    "                                              batch_size=CFG.bs,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=CFG.num_workers,\n",
    "                                              sampler=None,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              pin_memory=False,\n",
    "                                              drop_last=True),\n",
    "                                  fp16=True)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=1,shuffle=False,\n",
    "                              num_workers=1,pin_memory=False)\n",
    "    test_dataloader = DataLoader(test_dataset, \n",
    "                                  batch_size=1,shuffle=False,\n",
    "                                  num_workers=1,pin_memory=False)\n",
    "\n",
    "    model = UNet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=1,\n",
    "                channels=(8, 16, 32, 64),\n",
    "                strides=(1, 1, 1)\n",
    "                )\n",
    "    #model = UNETR(spatial_dims=3,in_channels=1, out_channels=1, img_size=(128,128,128), \n",
    "    #              hidden_size=1024,num_heads=4,feature_size=32, norm_name='batch')\n",
    "    model.to(CFG.device)\n",
    "    \n",
    "    optim_dict = dict(optim=CFG.optim,lr=CFG.lr,weight_decay=CFG.wd)\n",
    "    optimizer = get_optimizer(model, **optim_dict)\n",
    "    \n",
    "    #scheduler = CFG.scheduler(optimizer, lr_lambda=lambda epoch: CFG.scheduler_multi_lr_fact )\n",
    "    if CFG.scheduler==torch.optim.lr_scheduler.OneCycleLR:\n",
    "        scheduler = CFG.scheduler(optimizer, max_lr=CFG.lr, steps_per_epoch=len(train_loader)//CFG.accumulation_steps, epochs=CFG.epochs)\n",
    "    elif CFG.scheduler==\"warmup_cosine\":\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(optimizer,\n",
    "                                                  warmup_epochs=CFG.warmup_epochs,\n",
    "                                                  max_epochs=CFG.epochs)\n",
    "    criterion = CFG.crit    \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"from torch_lr_finder import LRFinder\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=100, num_iter=100)\n",
    "    lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "    lr_finder.reset()\"\"\"\n",
    "\n",
    "    \n",
    "    trainer = Trainer(CFG,\n",
    "                      model=model, \n",
    "                      device=CFG.device, \n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      criterion=criterion,\n",
    "                      writer=writer,\n",
    "                      fold=fold,\n",
    "                      max_norm=CFG.max_norm,\n",
    "                      mixup_fn=mixup_fn)\n",
    "    \n",
    "    history = trainer.fit(\n",
    "            CFG.epochs, \n",
    "            train_loader, \n",
    "            valid_loader, \n",
    "            f\"{result_dir}/\", \n",
    "            CFG.epochs,\n",
    "        )\n",
    "    trainer.test(test_dataloader,result_dir)\n",
    "    td_sdice = get_target_domain_metrics(CFG.dataset_path,Path(CFG.results_dir),fold)\n",
    "    #writer.add_hparams(class2dict(CFG),td_sdice)\n",
    "    wandb.log(td_sdice)\n",
    "    writer.close()\n",
    "    run.finish()\n",
    "\n",
    "    del trainer\n",
    "    del train_loader\n",
    "    del valid_loader\n",
    "    del train_dataset\n",
    "    del valid_dataset\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6792c846-9635-4527-816f-6c6349cf80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmklasen\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mklasen/domain_shift/runs/et0x7h8o\" target=\"_blank\">mode_3</a></strong> to <a href=\"https://wandb.ai/mklasen/domain_shift\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching Train Data ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='56' class='' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [56/56 00:19<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4/4 00:01<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='49' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      49.00% [49/100 2:21:14<2:26:59]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>train/dice</th>\n",
       "      <th>valid/loss</th>\n",
       "      <th>valid/dice</th>\n",
       "      <th>valid/sdice</th>\n",
       "      <th>LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.4837</td>\n",
       "      <td>0.3990</td>\n",
       "      <td>6.5246</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.8271</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.2453</td>\n",
       "      <td>0.3392</td>\n",
       "      <td>3.5465</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.0814</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.0327</td>\n",
       "      <td>0.5203</td>\n",
       "      <td>2.6271</td>\n",
       "      <td>0.1274</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.1807</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>1.9135</td>\n",
       "      <td>0.1906</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.6528</td>\n",
       "      <td>0.7671</td>\n",
       "      <td>1.5327</td>\n",
       "      <td>0.2382</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.2544</td>\n",
       "      <td>0.8304</td>\n",
       "      <td>1.2470</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.2784</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.0555</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>1.0797</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.2567</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>0.3492</td>\n",
       "      <td>0.2962</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.6856</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.8107</td>\n",
       "      <td>0.3615</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.6863</td>\n",
       "      <td>0.3362</td>\n",
       "      <td>0.3526</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.5313</td>\n",
       "      <td>0.8959</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.2873</td>\n",
       "      <td>0.3356</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>0.9097</td>\n",
       "      <td>0.5646</td>\n",
       "      <td>0.3759</td>\n",
       "      <td>0.3218</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>0.6346</td>\n",
       "      <td>0.4346</td>\n",
       "      <td>0.3433</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.3793</td>\n",
       "      <td>0.9122</td>\n",
       "      <td>0.4872</td>\n",
       "      <td>0.4046</td>\n",
       "      <td>0.3843</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.3192</td>\n",
       "      <td>0.9254</td>\n",
       "      <td>0.4478</td>\n",
       "      <td>0.5102</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.3093</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.5529</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.3890</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>0.3856</td>\n",
       "      <td>0.5332</td>\n",
       "      <td>0.4502</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.2517</td>\n",
       "      <td>0.9360</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.4866</td>\n",
       "      <td>0.4498</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.2404</td>\n",
       "      <td>0.9384</td>\n",
       "      <td>0.3460</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.4787</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>0.9431</td>\n",
       "      <td>0.3699</td>\n",
       "      <td>0.5430</td>\n",
       "      <td>0.4599</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.9399</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.4611</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.2097</td>\n",
       "      <td>0.9452</td>\n",
       "      <td>0.3537</td>\n",
       "      <td>0.4858</td>\n",
       "      <td>0.4892</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>0.5359</td>\n",
       "      <td>0.4942</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.2066</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3104</td>\n",
       "      <td>0.5414</td>\n",
       "      <td>0.5127</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.9491</td>\n",
       "      <td>0.3226</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.5115</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.2015</td>\n",
       "      <td>0.9465</td>\n",
       "      <td>0.2682</td>\n",
       "      <td>0.5469</td>\n",
       "      <td>0.5601</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>0.6377</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>0.9539</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.5569</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.9534</td>\n",
       "      <td>0.3254</td>\n",
       "      <td>0.4876</td>\n",
       "      <td>0.5218</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.1584</td>\n",
       "      <td>0.9572</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.5608</td>\n",
       "      <td>0.5515</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.2267</td>\n",
       "      <td>0.5987</td>\n",
       "      <td>0.5943</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.1529</td>\n",
       "      <td>0.9585</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>0.9582</td>\n",
       "      <td>0.2553</td>\n",
       "      <td>0.5919</td>\n",
       "      <td>0.5519</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.6515</td>\n",
       "      <td>0.6168</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.1518</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.2558</td>\n",
       "      <td>0.6301</td>\n",
       "      <td>0.5695</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.1533</td>\n",
       "      <td>0.9585</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.5533</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.1532</td>\n",
       "      <td>0.9582</td>\n",
       "      <td>0.2629</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.1546</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.5203</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.1501</td>\n",
       "      <td>0.9591</td>\n",
       "      <td>0.3361</td>\n",
       "      <td>0.4830</td>\n",
       "      <td>0.5488</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.9606</td>\n",
       "      <td>0.2427</td>\n",
       "      <td>0.5973</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.5729</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>0.9598</td>\n",
       "      <td>0.2404</td>\n",
       "      <td>0.5951</td>\n",
       "      <td>0.5628</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.1345</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.6142</td>\n",
       "      <td>0.6006</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.1384</td>\n",
       "      <td>0.9624</td>\n",
       "      <td>0.2042</td>\n",
       "      <td>0.6238</td>\n",
       "      <td>0.6236</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.1389</td>\n",
       "      <td>0.9622</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.5784</td>\n",
       "      <td>0.6089</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.9625</td>\n",
       "      <td>0.1874</td>\n",
       "      <td>0.6784</td>\n",
       "      <td>0.6460</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.9646</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.6272</td>\n",
       "      <td>0.6356</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.2051</td>\n",
       "      <td>0.6288</td>\n",
       "      <td>0.6108</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      23.21% [13/56 00:37<02:04 train_loss: 0.1212, LR: 5.58e-04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39202/1039165449.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_39202/803187213.py\u001b[0m in \u001b[0;36mrun_fold\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m    112\u001b[0m                       mixup_fn=mixup_fn)\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     history = trainer.fit(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mlk/New Volume/Lab/domain_shift_anatomy/experimental/volume_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, train_loader, valid_loader, save_path, patience)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;31m#train_loss, train_dice, train_sdice = 0,0,0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sdice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mlk/New Volume/Lab/domain_shift_anatomy/experimental/volume_trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader, mb, n_epoch)\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch1.9/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch1.9/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold in CFG.fold:\n",
    "    run_fold(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577617b1-afc3-4542-989f-7bf168723d8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c18161-9314-4ac7-a99b-b3ef021f7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "seed = 0xBadCafe\n",
    "val_size = 4\n",
    "n_experiments = len(cc359_df.fold.unique())\n",
    "split = one2all(df=cc359_df,val_size=val_size)[:n_experiments]\n",
    "train_df = cc359_df.iloc[split[0][0]].reset_index()\n",
    "\n",
    "sa_x,sa_y = create_3d_shared_arrays(CFG,train_df,root_dir=CFG.dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cffff-a42e-431d-b387-d4940649bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CC359_Dataset(CFG,df=train_df,root_dir=CFG.dataset_path,\n",
    "                              voxel_spacing=CFG.voxel_spacing,transforms=get_transforms(\"rrc\"),\n",
    "                              mode=\"train\", cache=True, cached_x=sa_x, cached_y=sa_y)\n",
    "train_loader = PrefetchLoader(DataLoader(train_dataset,\n",
    "                                              batch_size=CFG.bs,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=CFG.num_workers,\n",
    "                                              sampler=None,\n",
    "                                              collate_fn=None,\n",
    "                                              pin_memory=False,\n",
    "                                              drop_last=True),\n",
    "                                  fp16=True)\n",
    "for x,y in train_loader:\n",
    "    print(x.size(),y.size())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
