{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa4d85-7079-4309-8a7b-b5ab7f023445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spottunet.dataset.cc359 import *\n",
    "from spottunet.split import one2one\n",
    "from models.spottune_unet import UNet2D\n",
    "from models.resnet import resnet\n",
    "from spottunet.utils import sdice\n",
    "from dpipe.im.metrics import dice_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler \n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from monai import transforms as T\n",
    "from monai.transforms import Compose, apply_transform\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "\n",
    "import json\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from dpipe.im.shape_ops import zoom\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEBUG=False\n",
    "\n",
    "torch.cuda.set_device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe6672-8878-41b3-92f4-2afcbb98d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from configs.config_spottune import CFG\n",
    "from utils import *\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "from dataset.dataloader import *\n",
    "from dataset.loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75e2ba-5f56-47cc-90c9-b0b9803350ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_spottune import SpotTuneTrainer\n",
    "from dataset.dataloader import *\n",
    "from dataset.loader import *\n",
    "from dataset.dataloader_utils import *\n",
    "from dataset.augment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966272de-6331-4949-a46d-d6afb5e15e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "    seed = 0xBadCafe\n",
    "    pretrained = True\n",
    "    n_first_exclude = 5\n",
    "    n_exps = 30\n",
    "    split = one2one(cc359_df, val_size=CFG.val_size, n_add_ids=CFG.n_add_ids,\n",
    "                train_on_add_only=pretrained, seed=seed)[n_first_exclude:n_exps]\n",
    "    train_df = cc359_df.iloc[split[0][0]].reset_index()\n",
    "\n",
    "    sa_x,sa_y = create_shared_arrays(CFG,train_df,root_dir=CFG.dataset_path)\n",
    "    train_dataset = CC359_Dataset(CFG,df=train_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=tfms,\n",
    "                                  mode=\"train\", cache=True, cached_x=sa_x, cached_y=sa_y)\n",
    "    for x,y in train_dataset:\n",
    "        plt.imshow(x.squeeze(), \"gray\")\n",
    "        plt.show()\n",
    "        plt.imshow(y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f65e4e-a2de-4d9d-8017-368748726cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup2k_reg_opt = {3: {1: 0.000, }, 2: {1: 0.005, 3: 0.007, 6: 0.005, 12: 0.005, 24: 0.012, 36: 0.007, 48: 0.010 },\n",
    "                   1: {1: 0.005, 3: 0.007, 6: 0.005, 12: 0.005, 24: 0.012, 36: 0.007, 48: 0.010}}\n",
    "\n",
    "def run_fold(fold):\n",
    "    result_dir = CFG.results_dir + \"/mode_\"+str(fold)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    #wandb.tensorboard.patch(root_logdir=result_dir+\"/logs\")\n",
    "    run = wandb.init(project=\"domain_shift\",\n",
    "                     group=CFG.model_name,\n",
    "                     name=f\"mode_{str(fold)}\",\n",
    "                     job_type=\"spottune_gamma\",\n",
    "                     config=class2dict(CFG),\n",
    "                     reinit=True,\n",
    "                     sync_tensorboard=True)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=result_dir+\"/logs\")\n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "    \n",
    "    k_reg=setup2k_reg_opt[CFG.n_add_ids][CFG.slice_sampling_interval]\n",
    "    seed = 0xBadCafe\n",
    "    pretrained = True\n",
    "    n_first_exclude = 5\n",
    "    n_exps = 30\n",
    "    split = one2one(cc359_df, val_size=CFG.val_size, n_add_ids=CFG.n_add_ids,\n",
    "                train_on_add_only=pretrained, seed=seed)[n_first_exclude:n_exps]\n",
    "        \n",
    "    train_df = cc359_df.iloc[split[fold][0]].reset_index()\n",
    "    valid_df = cc359_df.iloc[split[fold][1]].reset_index()\n",
    "    test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "    print(\"Caching Train Data ...\")\n",
    "    \n",
    "    sa_x,sa_y = create_shared_arrays(CFG,train_df,root_dir=CFG.dataset_path)\n",
    "    train_dataset = CC359_Dataset(CFG,df=train_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,transforms=get_transforms(CFG.tfms),\n",
    "                                  mode=\"train\", cache=True, cached_x=sa_x, cached_y=sa_y)\n",
    "    \n",
    "    valid_dataset = CC359_Dataset(CFG,df=valid_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,\n",
    "                                  transforms=get_transforms(\"default\"),mode=\"val\", cache=False)\n",
    "    test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,\n",
    "                                  transforms=get_transforms(\"default\"),mode=\"test\", cache=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                                              batch_size=CFG.bs,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=CFG.num_workers,\n",
    "                                              sampler=None,\n",
    "                                              collate_fn=fast_collate,\n",
    "                                              pin_memory=False)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=1,shuffle=False,\n",
    "                              num_workers=1,pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, \n",
    "                                  batch_size=1,shuffle=False,\n",
    "                                  num_workers=1,pin_memory=False)\n",
    "    \n",
    "    model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "    \n",
    "    model_policy = resnet(num_class=64)\n",
    "    \n",
    "    load_model_state_fold_wise(architecture=model, baseline_exp_path=CFG.baseline_exp_path, exp=fold,\n",
    "                               modify_state_fn=modify_state_fn_spottune, n_folds=len(cc359_df.fold.unique()),\n",
    "                               n_first_exclude=n_first_exclude),\n",
    "    freeze_model_spottune(model)\n",
    "    \n",
    "    model.to(CFG.device)\n",
    "    model_policy.to(CFG.device)\n",
    "    \n",
    "    # Optims\n",
    "    optim_dict = dict(optim=CFG.optim,lr=CFG.lr,weight_decay=CFG.wd)\n",
    "    optimizer_main = get_optimizer(model, **optim_dict)\n",
    "    optimizer_policy = get_optimizer(model_policy, **optim_dict)\n",
    "        # Scheduler\n",
    "    if CFG.scheduler==torch.optim.lr_scheduler.OneCycleLR:\n",
    "        steps = max(len(train_loader),1)\n",
    "        scheduler_main = CFG.scheduler(optimizer_main, max_lr=CFG.lr, \n",
    "                                       steps_per_epoch=steps,\n",
    "                                       epochs=CFG.epochs)\n",
    "        scheduler_policy = CFG.scheduler(optimizer_policy, max_lr=CFG.lr, \n",
    "                                       steps_per_epoch=steps, \n",
    "                                       epochs=CFG.epochs)\n",
    "    elif CFG.scheduler==torch.optim.lr_scheduler.MultiplicativeLR:\n",
    "        scheduler_main = CFG.scheduler(optimizer_main, lr_lambda=lambda epoch: CFG.scheduler_multi_lr_fact )\n",
    "        scheduler_policy = CFG.scheduler(optimizer_policy, lr_lambda=lambda epoch: CFG.scheduler_multi_lr_fact )\n",
    "    \n",
    "    criterion = CFG.crit\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    trainer = SpotTuneTrainer(CFG,\n",
    "                      model,\n",
    "                      model_policy,\n",
    "                      CFG.device, \n",
    "                      optimizer_main,\n",
    "                      scheduler_main,\n",
    "                      optimizer_policy,\n",
    "                      scheduler_policy,\n",
    "                      criterion,writer,fold,\n",
    "                      CFG.max_norm,\n",
    "                      CFG.temperature,k_reg,CFG.reg_mode)\n",
    "    \n",
    "    history = trainer.fit(\n",
    "            CFG.epochs, \n",
    "            train_loader, \n",
    "            valid_loader, \n",
    "            f\"{result_dir}/\", \n",
    "            CFG.epochs,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    del train_loader\n",
    "    del valid_loader\n",
    "    del train_dataset\n",
    "    del valid_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    trainer.test(test_dataloader,result_dir)\n",
    "    td_sdice = get_target_domain_metrics(CFG.dataset_path,Path(CFG.results_dir),fold)\n",
    "    #wandb.log(td_sdice)\n",
    "    writer.close()\n",
    "    #run.finish()\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d916a81-a1ec-4a14-a25e-cff2cb2d3ef4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fold in CFG.fold:\n",
    "    run_fold(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537f4a8-c98a-43bf-9937-f0fc87f94bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_slice(arrays, interval: int = 1):\n",
    "    slc = np.random.randint(arrays[0].shape[-1] // interval) * interval\n",
    "    return tuple(array[..., slc] for array in arrays)\n",
    "\n",
    "array = np.zeros((5,256,256,256))\n",
    "interval = 48\n",
    "arrays = get_random_slice(array,interval)\n",
    "slc = np.random.randint(arrays[0].shape[-1] // interval) * interval\n",
    "slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caaa611-b5d7-4608-adff-6856dfe8ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 48 96 144 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26423d-0c0d-4e18-9f2b-db1433f96d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "192+48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c2d56-ea32-4549-8b3f-7df461476030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_run(fold):\n",
    "    result_dir = CFG.results_dir + \"/mode_\"+str(fold)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "\n",
    "    model = UNet2D(n_chans_in=CFG.n_chans_in, n_chans_out=CFG.n_chans_out, n_filters_init=CFG.n_filters)\n",
    "    model.load_state_dict(torch.load(f\"{result_dir}/mode_{fold}_best_epoch_model.pth\")[\"model_state_dict\"])    \n",
    "    freeze_model_spottune(model)\n",
    "    model.to(CFG.device)\n",
    "    \n",
    "    model_policy = resnet(num_class=64)\n",
    "    model_policy.to(CFG.device)\n",
    "    model_policy.load_state_dict(torch.load(f\"{result_dir}/mode_{fold}_best_epoch_model.pth\")[\"model_policy_state_dict\"])  \n",
    "\n",
    "\n",
    "    seed = 0xBadCafe\n",
    "    pretrained = True\n",
    "    n_first_exclude = 5\n",
    "    n_exps = 30\n",
    "    split = one2one(cc359_df, val_size=CFG.val_size, n_add_ids=CFG.n_add_ids,\n",
    "                train_on_add_only=pretrained, seed=seed)[n_first_exclude:n_exps]\n",
    "\n",
    "    test_df  = cc359_df.iloc[split[fold][2]].reset_index()\n",
    "\n",
    "    test_dataset = CC359_Dataset(CFG,df=test_df,root_dir=CFG.dataset_path,\n",
    "                                  voxel_spacing=CFG.voxel_spacing,\n",
    "                                  transforms=None,mode=\"test\", cache=False)\n",
    "    trainer = SpotTuneTrainer(CFG,\n",
    "                              model,\n",
    "                              model_policy,\n",
    "                              CFG.device, \n",
    "                              None,\n",
    "                              None,\n",
    "                              None,\n",
    "                              None,\n",
    "                              None,\n",
    "                              None,fold,\n",
    "                              CFG.max_norm,\n",
    "                              CFG.temperature,CFG.k_reg,CFG.reg_mode)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, \n",
    "                                  batch_size=1,shuffle=False,\n",
    "                                  num_workers=1,pin_memory = False)\n",
    "\n",
    "    trainer.test(test_dataloader, result_dir)\n",
    "    td_sdice = get_target_domain_metrics(CFG.dataset_path,Path(CFG.results_dir),fold)\n",
    "    print(td_sdice)\n",
    "    return td_sdice\n",
    "\n",
    "#test_run(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dab6f1-be0f-4b22-b1df-bbaad9fd92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc359_df = pd.read_csv(f\"{CFG.dataset_path}/meta.csv\",delimiter=\",\")\n",
    "seed = 0xBadCafe\n",
    "pretrained = True\n",
    "n_first_exclude = 5\n",
    "n_exps = 30\n",
    "split = one2one(cc359_df, val_size=CFG.val_size, n_add_ids=CFG.n_add_ids,\n",
    "            train_on_add_only=pretrained, seed=seed)[n_first_exclude:n_exps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6df96-e778-422e-8a53-b4438618e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in np.arange(0,25):\n",
    "    model = cc359_df.iloc[split[i][0]]['tomograph_model'].unique()\n",
    "    tsla = cc359_df.iloc[split[i][0]]['tesla_value'].unique()\n",
    "    print(str(model),str(tsla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a22c9-5d80-4d6b-9d99-058d521f4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    Siemens15\n",
    "Siemens3\n",
    "    Ge15\n",
    "    ge3\n",
    "    philips15\n",
    "    philips3\n",
    "\n",
    "    siemens15\n",
    "    siemens3\n",
    "ge15\n",
    "    ge3\n",
    "    philips15\n",
    "    philips3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b85ca-0b15-450e-b826-44b2423ba2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "baseline = \"baseline_results/baseline23_normed\"\n",
    "mode_domain = [\"Siemens15\", \"Ge15\", \"Ge3\", \"philips15\", \"philips3\"]\n",
    "#paths = [\"exp_03_no_aug\", \"exp_03_rrc\", \"exp_03_gaussianblur\", \"exp_03_ssi_48_nid_1_gamma\"]\n",
    "paths = [\"exp_ssi_48_nid_1_norm\"]\n",
    "results = []\n",
    "for path in paths:\n",
    "    mode_dict = {}\n",
    "    with open(f\"spottune_results/{path}/mode_{str(3)}/sdice_score.json\",\"r\") as f:\n",
    "            data = json.load(f)\n",
    "            mode_dict[mode_domain[mode]] = np.mean(list(data.values()))\n",
    "    modes = [3]\n",
    "    for mode in modes:\n",
    "        \n",
    "        with open(f\"spottune_results/{path}/mode_{str(mode)}/sdice_score.json\",\"r\") as f:\n",
    "            data = json.load(f)\n",
    "            mode_dict[mode_domain[mode]] = np.mean(list(data.values()))\n",
    "    mode_dict[' '] = path\n",
    "    results.append(mode_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffbf49-9a8f-4a0b-84ff-bed7804d4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(results, index=' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470761a-d154-4d90-a075-742a10066513",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df.iloc[0:-1] - df.iloc[-1], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
